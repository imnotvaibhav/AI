{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen\n",
      "  Using cached pyautogen-0.2.28-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: diskcache in c:\\python311\\lib\\site-packages (from pyautogen) (5.6.3)\n",
      "Requirement already satisfied: docker in c:\\python311\\lib\\site-packages (from pyautogen) (7.1.0)\n",
      "Requirement already satisfied: flaml in c:\\python311\\lib\\site-packages (from pyautogen) (2.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in c:\\python311\\lib\\site-packages (from pyautogen) (1.24.3)\n",
      "Requirement already satisfied: openai>=1.3 in c:\\python311\\lib\\site-packages (from pyautogen) (1.34.0)\n",
      "Requirement already satisfied: packaging in c:\\python311\\lib\\site-packages (from pyautogen) (23.2)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in c:\\python311\\lib\\site-packages (from pyautogen) (2.5.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\python311\\lib\\site-packages (from pyautogen) (1.0.0)\n",
      "Requirement already satisfied: termcolor in c:\\python311\\lib\\site-packages (from pyautogen) (2.3.0)\n",
      "Requirement already satisfied: tiktoken in c:\\python311\\lib\\site-packages (from pyautogen) (0.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python311\\lib\\site-packages (from openai>=1.3->pyautogen) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python311\\lib\\site-packages (from openai>=1.3->pyautogen) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python311\\lib\\site-packages (from openai>=1.3->pyautogen) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\python311\\lib\\site-packages (from openai>=1.3->pyautogen) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python311\\lib\\site-packages (from openai>=1.3->pyautogen) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\python311\\lib\\site-packages (from openai>=1.3->pyautogen) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\python311\\lib\\site-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.14.6)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\python311\\lib\\site-packages (from docker->pyautogen) (306)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python311\\lib\\site-packages (from docker->pyautogen) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\python311\\lib\\site-packages (from docker->pyautogen) (1.26.16)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python311\\lib\\site-packages (from tiktoken->pyautogen) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.26.0->docker->pyautogen) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>4->openai>=1.3->pyautogen) (0.4.6)\n",
      "Using cached pyautogen-0.2.28-py3-none-any.whl (284 kB)\n",
      "Installing collected packages: pyautogen\n",
      "Successfully installed pyautogen-0.2.28\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyautogen\n",
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Downloaded **Mistral-7B-Instruct-v0.1-GGUF** LLM via LM Studio from Hugging Face.\n",
    "\n",
    "* Deployed LLM model on local server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33massistant\u001b[0m (to assistant):\n",
      "\n",
      "Burr, tell me a joke\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33massistant\u001b[0m (to assistant):\n",
      "\n",
      " Sure thing! Here's one for you:\n",
      "\n",
      "Why did the tomato turn red?\n",
      "\n",
      "Because it saw the salad dressing!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33massistant\u001b[0m (to assistant):\n",
      "\n",
      " That's a classic, Timmy. Do you have any other good ones up your sleeve?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33massistant\u001b[0m (to assistant):\n",
      "\n",
      " Of course I do! How about this one:\n",
      "\n",
      "Why was the math book sad?\n",
      "\n",
      "Because it had too many problems!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mistral={\n",
    "\"config_list\":[{\n",
    "    \"model\":\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    \"base_url\":\"http://localhost:1234/v1\",\n",
    "    \"api_key\":\"lm-studio\"\n",
    "},],\n",
    "\"cache_seed\":None\n",
    "}\n",
    "\n",
    "\n",
    "Bill= ConversableAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=mistral,\n",
    "    system_message=\"You are Bill and you are a comedian.\"\n",
    ")\n",
    "\n",
    "\n",
    "Burr= ConversableAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=mistral,\n",
    "    system_message=\"You are Burr and you are a part of two person comedy show.\"\n",
    ")\n",
    "\n",
    "\n",
    "chat_result = Bill.initiate_chat(Burr,message=\"Burr, tell me a joke\",max_turns=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AssistantAgent and UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Tell me a Joke.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      " Sure! Here's a joke for you:\n",
      "\n",
      "Why was the math book sad? Because it had too many problems. \n",
      "\n",
      "I hope you found that joke amusing! Let me know if you need anything else.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Tell me a Joke.', 'role': 'assistant'}, {'content': \" Sure! Here's a joke for you:\\n\\nWhy was the math book sad? Because it had too many problems. \\n\\nI hope you found that joke amusing! Let me know if you need anything else.\", 'role': 'user'}], summary=\" Sure! Here's a joke for you:\\n\\nWhy was the math book sad? Because it had too many problems. \\n\\nI hope you found that joke amusing! Let me know if you need anything else.\", cost={'usage_including_cached_inference': {'total_cost': 0, 'TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q2_K.gguf': {'cost': 0, 'prompt_tokens': 498, 'completion_tokens': 47, 'total_tokens': 545}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q2_K.gguf': {'cost': 0, 'prompt_tokens': 498, 'completion_tokens': 47, 'total_tokens': 545}}}, human_input=['exit'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": mistral['config_list']})\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
    "user_proxy.initiate_chat(assistant, message=\"Tell me a Joke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupchat (Planner, Admin, Analyst, Engineer)\n",
    "\n",
    "\n",
    "Engineer: write code\n",
    "\n",
    "Analyst: Analyses and gets insights\n",
    "\n",
    "Planner: Suggests plan\n",
    "\n",
    "Executor: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral={\n",
    "\"config_list\":[{\n",
    "    \"model\":\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    \"base_url\":\"http://localhost:1234/v1\",\n",
    "    \"api_key\":\"lm-studio\"\n",
    "},],\n",
    "\"cache_seed\":None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=mistral,\n",
    "    system_message=\"\"\"Engineer. You follow an approved plan. You write python code to solve tasks. The user can't modify your code. Do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. \n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Analyst\",\n",
    "    llm_config=mistral,\n",
    "    system_message=\"\"\"Analyst. You follow an approved plan. You are able to draw conclusion from codes and graphs. You don't write code.\"\"\",\n",
    ")\n",
    "#########################################\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"Planner. Suggest a plan. Revise the plan based on feedback from admin, until admin approval.\n",
    "The plan may involve an engineer who can write code and an Analyst who doesn't write code.\n",
    "Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a Analyst.\n",
    "\"\"\",\n",
    "    llm_config=mistral,\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"paper\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor], messages=[], max_round=30\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to Planner):\n",
      "\n",
      "\n",
      "Should a businessman invest in Apple stocks? Suggest based on time series forecast analysis of Apple stocks using fbprophet library. If stock close prices are increasing for next 5 days, invest in Apple stocks, otherwise don't.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to Admin):\n",
      "\n",
      " Plan:\n",
      "\n",
      "1. Collect historical data for Apple stock prices over a specified period of time (e.g., 1 year).\n",
      "2. Clean and preprocess the data using pandas library.\n",
      "3. Train a time-series model using fbprophet library on the preprocessed data to predict future stock prices.\n",
      "4. Use the predictions to determine if the stock prices are increasing for the next 5 days.\n",
      "5. If the stock prices are predicted to increase, recommend investing in Apple stocks. Otherwise, do not recommend investing.\n",
      "6. Review the plan and make any necessary revisions based on feedback from admin.\n",
      "\n",
      "Roles:\n",
      "\n",
      "1. Engineer - responsible for writing code for data collection, preprocessing, and time-series model training.\n",
      "2. Analyst - responsible for reviewing the plan and providing feedback for revision.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAdmin\u001b[0m (to Planner):\n",
      "\n",
      "Engineer starts working on his role. Then Analyst will analyze the results.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    planner,\n",
    "    message=\"\"\"\n",
    "Should a businessman invest in Apple stocks? Suggest based on time series forecast analysis of Apple stocks using fbprophet library. If stock close prices are increasing for next 5 days, invest in Apple stocks, otherwise don't.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################\n",
    "##                                                      System heated                                                                 ##\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Implementation in Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task='qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 06-14 22:47:12] {129} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deccc13d0b564a8d8442f54b465588b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e74a92ba70b4378b2b014b57565e79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850e7b9062ae4de489364f945acd4fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5d01c3869d4505baf1d1f9daa5fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7dee72ca7842ea914f00bad92aea9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf96ec64b6364029b5f5a4b5738e2470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9389fb1ac05a4fec898dc3fac3f99a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c98a77318474e28bf6c133fcb671952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5febf0036144412793a2ab053b17e70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7389668fc764e698c441f1b59c86f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7481e9eba41249f29531e858acc4d61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "    },\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 01:23:24,770 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - \u001b[32mUse the existing collection `autogen-docs`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 01:23:25,919 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 1 chunks.\u001b[0m\n",
      "Number of requested results 20 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB returns doc_ids:  [['2bea6e64', '36d6eefd', '600897af']]\n",
      "\u001b[32mAdding content of doc 2bea6e64 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: What is autogen?\n",
      "\n",
      "Context is: # Getting Started | AutoGen\n",
      "\n",
      "Getting Started | AutoGen\n",
      "\n",
      "[Skip to main content](#__docusaurus_skipToContent_fallback)What's new in AutoGen? Read [this blog](/autogen/blog/2024/03/03/AutoGen-Update) for an overview of updates[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](#)* [Getting Started](/autogen/docs/Getting-Started)\n",
      "* [Installation](/autogen/docs/installation/)\n",
      "* [Tutorial](/autogen/docs/tutorial/introduction)\n",
      "* [User Guide](/autogen/docs/topics)\n",
      "* [API Reference](/autogen/docs/reference/agentchat/conversable_agent)\n",
      "* [FAQs](/autogen/docs/FAQ)\n",
      "* [AutoGen Studio](/autogen/docs/autogen-studio/getting-started)\n",
      "* [Ecosystem](/autogen/docs/ecosystem)\n",
      "* [Contributor Guide](/autogen/docs/contributor-guide/contributing)\n",
      "* [Research](/autogen/docs/Research)\n",
      "[Examples](#)* [Examples by Category](/autogen/docs/Examples)\n",
      "* [Examples by Notebook](/autogen/docs/notebooks)\n",
      "* [Application Gallery](/autogen/docs/Gallery)\n",
      "[Other Languages](#)* [Dotnet](https://microsoft.github.io/autogen-for-net/)\n",
      "[Blog](/autogen/blog)[GitHub](https://github.com/microsoft/autogen)[Discord](https://aka.ms/autogen-dc)[Twitter](https://twitter.com/pyautogen)`ctrl``K`* [Getting Started](/autogen/docs/Getting-Started)\n",
      "* [Installation](/autogen/docs/installation/)\n",
      "* [Tutorial](/autogen/docs/tutorial)\n",
      "\t+ [Introduction](/autogen/docs/tutorial/introduction)\n",
      "\t+ [Chat Termination](/autogen/docs/tutorial/chat-termination)\n",
      "\t+ [Human in the Loop](/autogen/docs/tutorial/human-in-the-loop)\n",
      "\t+ [Code Executors](/autogen/docs/tutorial/code-executors)\n",
      "\t+ [Tool Use](/autogen/docs/tutorial/tool-use)\n",
      "\t+ [Conversation Patterns](/autogen/docs/tutorial/conversation-patterns)\n",
      "\t+ [What Next?](/autogen/docs/tutorial/what-next)\n",
      "* [Use Cases](/autogen/docs/Use-Cases/agent_chat)\n",
      "* [User Guide](/autogen/docs/topics)\n",
      "\t+ [Code Execution](/autogen/docs/topics/code-execution/cli-code-executor)\n",
      "\t+ [OpenAI Assistant](/autogen/docs/topics/openai-assistant/gpt_assistant_agent)\n",
      "\t+ [GroupChat](/autogen/docs/topics/groupchat/customized_speaker_selection)\n",
      "\t+ [Using Non-OpenAI Models](/autogen/docs/topics/non-openai-models/about-using-nonopenai-models)\n",
      "\t+ [Handling Long Contexts](/autogen/docs/topics/handling_long_contexts/compressing_text_w_llmligua)\n",
      "\t+ [LLM Caching](/autogen/docs/topics/llm-caching)\n",
      "\t+ [LLM Observability](/autogen/docs/topics/llm-observability)\n",
      "\t+ [LLM Configuration](/autogen/docs/topics/llm_configuration)\n",
      "\t+ [Prompting and Reasoning](/autogen/docs/topics/prompting-and-reasoning/react)\n",
      "\t+ [Retrieval Augmentation](/autogen/docs/topics/retrieval_augmentation)\n",
      "\t+ [Task Decomposition](/autogen/docs/topics/task_decomposition)\n",
      "* [API Reference](/autogen/docs/reference/agentchat/conversable_agent)\n",
      "* [FAQs](/autogen/docs/FAQ)\n",
      "* [AutoGen Studio](/autogen/docs/autogen-studio)\n",
      "* [Ecosystem](/autogen/docs/ecosystem)\n",
      "* [Contributor Guide](/autogen/docs/contributor-guide)\n",
      "* [Research](/autogen/docs/Research)\n",
      "* [Migration Guide](/autogen/docs/Migration-Guide)\n",
      "* \n",
      "* Getting Started\n",
      "On this pageGetting Started\n",
      "===============\n",
      "\n",
      "AutoGen is a framework that enables development of LLM applications using\n",
      "multiple agents that can converse with each other to solve tasks. AutoGen agents\n",
      "are customizable, conversable, and seamlessly allow human participation. They\n",
      "can operate in various modes that employ combinations of LLMs, human inputs, and\n",
      "tools.\n",
      "\n",
      "![AutoGen Overview](/autogen/assets/images/autogen_agentchat-250ca64b77b87e70d34766a080bf6ba8.png)\n",
      "\n",
      "### Main Features[​](#main-features \"Direct link to Main Features\")\n",
      "\n",
      "* AutoGen enables building next-gen LLM applications based on [multi-agent\n",
      "conversations](/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies\n",
      "the orchestration, automation, and optimization of a complex LLM workflow. It\n",
      "maximizes the performance of LLM models and overcomes their weaknesses.\n",
      "* It supports [diverse conversation\n",
      "patterns](/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns)\n",
      "for complex workflows. With customizable and conversable agents, developers can\n",
      "use AutoGen to build a wide range of conversation patterns concerning\n",
      "conversation autonomy, the number of agents, and agent conversation topology.\n",
      "* It provides a collection of working systems with different complexities. These\n",
      "systems span a [wide range of\n",
      "applications](/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen)\n",
      "from various domains and complexities. This demonstrates how AutoGen can\n",
      "easily support diverse conversation patterns.\n",
      "\n",
      "AutoGen is powered by collaborative [research studies](/autogen/docs/Research) from\n",
      "Microsoft, Penn State University, and University of Washington.\n",
      "\n",
      "### Quickstart[​](#quickstart \"Direct link to Quickstart\")\n",
      "\n",
      "```\n",
      "pip install pyautogen  \n",
      "\n",
      "```\n",
      "\n",
      "* No code execution\n",
      "* Local execution\n",
      "* Docker execution\n",
      "\n",
      "```\n",
      "import os  \n",
      "from autogen import AssistantAgent, UserProxyAgent  \n",
      "  \n",
      "llm_config = {\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}  \n",
      "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)  \n",
      "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)  \n",
      "  \n",
      "# Start the chat  \n",
      "user_proxy.initiate_chat(  \n",
      "    assistant,  \n",
      "    message=\"Tell me a joke about NVDA and TESLA stock prices.\",  \n",
      ")  \n",
      "\n",
      "```\n",
      "warningWhen asked, be sure to check the generated code before continuing to ensure it is safe to run.\n",
      "\n",
      "```\n",
      "import os  \n",
      "import autogen  \n",
      "from autogen import AssistantAgent, UserProxyAgent  \n",
      "  \n",
      "llm_config = {\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}  \n",
      "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)  \n",
      "  \n",
      "user_proxy = UserProxyAgent(  \n",
      "    \"user_proxy\", code_execution_config={\"executor\": autogen.coding.LocalCommandLineCodeExecutor(work_dir=\"coding\")}  \n",
      ")  \n",
      "  \n",
      "# Start the chat  \n",
      "user_proxy.initiate_chat(  \n",
      "    assistant,  \n",
      "    message=\"Plot a chart of NVDA and TESLA stock price change YTD.\",  \n",
      ")  \n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "import os  \n",
      "import autogen  \n",
      "from autogen import AssistantAgent, UserProxyAgent  \n",
      "  \n",
      "llm_config = {\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}  \n",
      "  \n",
      "with autogen.coding.DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:  \n",
      "    assistant = AssistantAgent(\"assistant\", llm_config=llm_config)  \n",
      "    user_proxy = UserProxyAgent(  \n",
      "        \"user_proxy\", code_execution_config={\"executor\": code_executor}  \n",
      "    )  \n",
      "  \n",
      "    # Start the chat  \n",
      "    user_proxy.initiate_chat(  \n",
      "        assistant,  \n",
      "        message=\"Plot a chart of NVDA and TESLA stock price change YTD. Save the plot to a file called plot.png\",  \n",
      "    )  \n",
      "\n",
      "```\n",
      "Open `coding/plot.png` to see the generated plot.\n",
      "\n",
      "tipLearn more about configuring LLMs for agents [here](/autogen/docs/topics/llm_configuration).\n",
      "\n",
      "#### Multi-Agent Conversation Framework[​](#multi-agent-conversation-framework \"Direct link to Multi-Agent Conversation Framework\")\n",
      "\n",
      "Autogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.\n",
      "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n",
      "\n",
      "The figure below shows an example conversation flow with AutoGen.\n",
      "\n",
      "![Agent Chat Example](/autogen/assets/images/chat_example-da70a7420ebc817ef9826fa4b1e80951.png)\n",
      "\n",
      "### Where to Go Next?[​](#where-to-go-next \"Direct link to Where to Go Next?\")\n",
      "\n",
      "* Go through the [tutorial](/autogen/docs/tutorial/introduction) to learn more about the core concepts in AutoGen\n",
      "* Read the examples and guides in the [notebooks section](/autogen/docs/notebooks)\n",
      "* Understand the use cases for [multi-agent conversation](/autogen/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/autogen/docs/Use-Cases/enhanced_inference)\n",
      "* Read the [API](/autogen/docs/reference/agentchat/conversable_agent/) docs\n",
      "* Learn about [research](/autogen/docs/Research) around AutoGen\n",
      "* Chat on [Discord](https://aka.ms/autogen-dc)\n",
      "* Follow on [Twitter](https://twitter.com/pyautogen)\n",
      "* See our [roadmaps](https://aka.ms/autogen-roadmap)\n",
      "\n",
      "If you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor's Guide](/autogen/docs/contributor-guide/contributing).\n",
      "\n",
      "[Edit this page](https://github.com/microsoft/autogen/edit/main/website/docs/Getting-Started.mdx)[NextInstallation](/autogen/docs/installation/)* [Main Features](#main-features)\n",
      "* [Quickstart](#quickstart)\n",
      "* [Where to Go Next?](#where-to-go-next)\n",
      "Community* [Discord](https://aka.ms/autogen-dc)\n",
      "* [Twitter](https://twitter.com/pyautogen)\n",
      "Copyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "assistant.reset()\n",
    "ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=\"What is autogen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task = \"code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral={\n",
    "\"config_list\":[{\n",
    "    \"model\":\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    \"base_url\":\"http://localhost:1234/v1\",\n",
    "    \"api_key\":\"lm-studio\"\n",
    "},],\n",
    "\"cache_seed\":None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=mistral,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\":\"https://github.com/aakash1104/Graph-Algorithms/README.md\"\n",
    "        \"get_or_create\":True\n",
    "    },\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "Write a code for Dijkstra's algorithm and explain the code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "assistant.reset()\n",
    "ragproxyagent.initiate_chat(assistant, problem=\"Write a code for Dijkstra's algorithm and explain the code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "##                      System heated up and crashed for last part                   ##\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
